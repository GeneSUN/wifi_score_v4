# ============================================================
# ğŸ§  MIND MAP â€” Workflow Overview of This Script
# ============================================================
#
# 1ï¸âƒ£  Initialization Phase
# â”œâ”€â”€ Import necessary libraries (PySpark, datetime, argparse, custom modules)
# â”œâ”€â”€ Append system path to include HourlyScore modules
# â”œâ”€â”€ Initialize SparkSession
# â””â”€â”€ Define HDFS paths:
#       â€¢ station_history_path
#       â€¢ station_connection_path
#       â€¢ station_score_output_path
#       â€¢ wifi_score_output_path
#
# 
# 2ï¸âƒ£  Argument Parsing
# â”œâ”€â”€ Use argparse to receive "--date_list" from shell script
# â”œâ”€â”€ If provided â†’ split into list of date_str values
# â””â”€â”€ If not provided â†’ automatically define:
#       â€¢ yesterday = today - 1 day
#       â€¢ today = current date
#       â†’ date_str_list = [yesterday, today]
#
#
# 3ï¸âƒ£  Missing Date/Hour Detection
# â”œâ”€â”€ For each date_str in date_str_list:
# â”‚     â”œâ”€â”€ Call find_missing_date_hours()
# â”‚     â”‚     â”œâ”€â”€ Determine which hours to check:
# â”‚     â”‚     â”‚     â€¢ If date_str == today â†’ check only hours < current hour
# â”‚     â”‚     â”‚     â€¢ Else â†’ check full 00â€“23 hours
# â”‚     â”‚     â”œâ”€â”€ For each hour:
# â”‚     â”‚     â”‚     â”œâ”€â”€ Construct both HDFS paths:
# â”‚     â”‚     â”‚     â”‚     â€¢ station_score_output_path/date=.../hour=...
# â”‚     â”‚     â”‚     â”‚     â€¢ wifi_score_output_path/date=.../hour=...
# â”‚     â”‚     â”‚     â”œâ”€â”€ Check if each path exists via Hadoop FileSystem
# â”‚     â”‚     â”‚     â”œâ”€â”€ If both exist â†’ log â€œ[OK]â€
# â”‚     â”‚     â”‚     â””â”€â”€ If either missing â†’ record in missing_list:
# â”‚     â”‚     â”‚           {date_str, hour_str, missing=[which ones missing]}
# â”‚     â”‚     â””â”€â”€ Return missing_list for that date
# â”‚     â””â”€â”€ Collect all missing results into all_missing[]
#
#
# 4ï¸âƒ£  Reprocessing Phase (for missing hours)
# â”œâ”€â”€ For each record in all_missing:
# â”‚     â”œâ”€â”€ Extract date_str, hour_str, missing
# â”‚     â”œâ”€â”€ Step 1: StationConnectionProcessor
# â”‚     â”‚     â€¢ Input: station_history_path, date_str, hour_str
# â”‚     â”‚     â€¢ Output: station_connection_df (cached)
# â”‚     â”œâ”€â”€ Step 2: station_score_hourly
# â”‚     â”‚     â€¢ Input: station_connection_df
# â”‚     â”‚     â€¢ Output written to station_score_output_path
# â”‚     â””â”€â”€ Step 3: wifi_score_hourly
# â”‚           â€¢ Input: same station_connection_df
# â”‚           â€¢ Output written to wifi_score_output_path
#
#
# 5ï¸âƒ£  Finalization
# â”œâ”€â”€ Print â€œ[DONE] All missing hours have been reprocessed successfully.â€
# â””â”€â”€ End Spark job
#
#
# ğŸ’¡ Summary of Logic Flow
# ------------------------------------------------------------
# Shell â†’ passes optional --date_list
# Python â†’ decides which dates to check (today/yesterday fallback)
# For each date â†’ find missing hourly outputs (station & wifi)
# For missing hours â†’ regenerate connection â†’ run scoring jobs
# Outputs written back to respective HDFS hourly directories
# ============================================================


from datetime import datetime, timedelta
import argparse
import sys
import time
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import (
    FloatType,
    IntegerType,
    StringType,
    StructType,
    Row,
)

# --------------------------------------------------------
# Custom Imports
# --------------------------------------------------------
sys.path.append('/usr/apps/vmas/script/ZS/HourlyScore')
from StationConnection import StationConnectionProcessor, LogTime
from station_score_hourly import station_score_hourly
from wifi_score_hourly import wifi_score_hourly


# ============================================================
# Function: Find Missing Hours for Both Station & WiFi Outputs
# ============================================================
def find_missing_date_hours(
    spark,
    date_str: str,
    station_score_output_path: str,
    wifi_score_output_path: str,
):
    """
    Identify which (date, hour) combinations have missing station_score or wifi_score outputs.

    Returns:
        List of dicts with keys: 'date_str', 'hour_str', 'missing'
    """
    hadoop_fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())

    # Determine which hours to check
    today_str = ( (datetime.utcnow()-timedelta(hours=1) ) ).strftime("%Y%m%d")
    if date_str == today_str:
        current_hour = (datetime.utcnow() - timedelta(hours=1)).hour
        hours_to_check = [f"{h:02d}" for h in range(current_hour)]  # only past hours
    else:
        hours_to_check = [f"{h:02d}" for h in range(24)]  # full 24 hours

    print(f"[INFO] Checking {len(hours_to_check)} hour folders for date={date_str}")

    missing_list = []
    for hour_str in hours_to_check:
        station_hour_path = f"{station_score_output_path}/date={date_str}/hour={hour_str}"
        wifi_hour_path = f"{wifi_score_output_path}/date={date_str}/hour={hour_str}"

        station_path_obj = spark._jvm.org.apache.hadoop.fs.Path(station_hour_path)
        wifi_path_obj = spark._jvm.org.apache.hadoop.fs.Path(wifi_hour_path)

        station_exists = hadoop_fs.exists(station_path_obj)
        wifi_exists = hadoop_fs.exists(wifi_path_obj)

        if not (station_exists and wifi_exists):
            missing = []
            if not station_exists:
                missing.append("station_score")
            if not wifi_exists:
                missing.append("wifi_score")

            missing_list.append({
                "date_str": date_str,
                "hour_str": hour_str,
                "missing": missing
            })
            print(f"[MISSING] {missing} for date={date_str}, hour={hour_str}")
        else:
            print(f"[OK] Both exist: date={date_str}, hour={hour_str}")

    return missing_list


# ============================================================
# Argument Parser
# ============================================================
def parse_args():
    parser = argparse.ArgumentParser(description="Hourly Score Checker")
    parser.add_argument(
        "--date_list",
        type=str,
        help='Space-separated list of dates, e.g. "20261015 20261016"',
        default=None,
    )
    return parser.parse_args()


# ============================================================
# Main Entry Point
# ============================================================
if __name__ == "__main__":

    spark = (
        SparkSession.builder
        .appName("backfill_wifi_score_hourly_report_v1")
        .config("spark.ui.port", "24045")
        .getOrCreate()
    )

    # HDFS Path Configuration
    hdfs_pd = "hdfs://njbbvmaspd11.nss.vzwnet.com:9000/"
    hdfs_pa = "hdfs://njbbepapa1.nss.vzwnet.com:9000"
    station_history_path = f"{hdfs_pa}/sha_data/StationHistory"
    device_groups_path = f"{hdfs_pa}/sha_data/DeviceGroups"
    station_connection_path = f"{hdfs_pa}/sha_data/hourlyScore_include_pac/station_connection_hourly"
    station_score_output_path = f"{hdfs_pa}/sha_data/hourlyScore_include_pac/station_score_hourly"
    wifi_score_output_path = f"{hdfs_pa}/sha_data/hourlyScore_include_pac/wifi_score_hourly"


    # Parse arguments
    args = parse_args()

    # Determine which dates to process
    if args.date_list:
        date_str_list = args.date_list.split()
        print(f"[INFO] Using provided date list: {date_str_list}")
    else:
        current_utc_hour =  (datetime.utcnow()-timedelta(hours=1) )
        today = (current_utc_hour).strftime("%Y%m%d")
        yesterday = (current_utc_hour- timedelta(days=1)).strftime("%Y%m%d")
        day_before_yesterday = (current_utc_hour- timedelta(days=2)).strftime("%Y%m%d")
        date_str_list = [yesterday, today]
        #date_str_list = [day_before_yesterday, yesterday, today]
        print(f"[INFO] No date list passed. Using default: {date_str_list}")

    # Find all missing date/hour combinations
    all_missing = []
    for date_str in date_str_list:
        missing_for_date = find_missing_date_hours(
            spark,
            date_str=date_str,
            station_score_output_path=station_score_output_path,
            wifi_score_output_path=wifi_score_output_path,
        )
        all_missing.extend(missing_for_date)

    # Reprocess missing date-hour combinations
    for d in all_missing:
        date_str, hour_str, missing = d["date_str"], d["hour_str"], d["missing"]
        print(f"\n[REPROCESS] date={date_str}, hour={hour_str}, missing={missing}")

        # Step 1: Generate station connection
        with LogTime() as timer:
            processor = StationConnectionProcessor(
                spark,
                input_path=station_history_path,
                date_str=date_str,
                hour_str=hour_str
            )
            station_connection_df = processor.run()
            station_connection_df.cache()

        # Step 2: Run station score
        with LogTime() as timer:
            station_runner = station_score_hourly(
                spark,
                date_str,
                hour_str,
                station_connection_df,
                output_path=station_score_output_path
            )
            station_runner.run()

        # Step 3: Run WiFi score
        with LogTime() as timer:

            wifi_score_runner = wifi_score_hourly(
                spark=spark,
                date_str=date_str,
                hour_str=hour_str,
                source_df=station_connection_df,
                station_history_path=station_history_path,
                device_groups_path=device_groups_path,
                output_path=wifi_score_output_path
            )
            wifi_score_runner.run()


    print("[DONE] All missing hours have been reprocessed successfully.")
